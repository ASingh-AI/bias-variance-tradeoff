# Biasâ€“Variance Tradeoff in Machine Learning

This project explores the **biasâ€“variance tradeoff** on the California Housing dataset.  
It demonstrates how **model complexity, feature engineering, and regularization** affect training vs test error.

---

## ğŸ”‘ Key Experiments

- **Complexity Sweep**  
  Polynomial degree â†‘ â†’ training error â†“, test error shows U-shaped curve (bias â†“, variance â†‘).

- **Feature Engineering**  
  Domain-informed features (rooms/person, bedroom ratio, coastal flag) outperform raw polynomial expansions.

- **Regularization (Ridge & Lasso)**  
  Prevents variance explosion at high degrees; optimal Î± balances bias and variance.

- **Learning Curves**  
  More data â†’ test error â†“ as variance shrinks.

- **Bootstrap Decomposition**  
  BiasÂ² â†“ and variance â†‘ with complexity; their sum tracks test MSE.

---

## ğŸ“Š Sample Results

| Experiment              | Plot |
|--------------------------|------|
| Complexity Sweep (OLS)   | ![Complexity](docs/figures/complexity_sweep.png) |
| Ridge Regularization     | ![Ridge](docs/figures/regularization_sweep.png) |
| Biasâ€“Variance Bootstrap  | ![Bootstrap](docs/figures/bias_variance_bootstrap.png) |

---

## âš™ï¸ How to Run

```bash
git clone https://github.com/<your-username>/bias-variance-california.git
cd bias-variance-california
pip install -r requirements.txt
python Bias_Variance_Spyder.py
